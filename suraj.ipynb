{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"suraj.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Psq2s15_wlfJ"},"source":["## Extract the Dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DuUurNXQtmXT","colab":{}},"source":["import zipfile  # For faster extraction\n","dataset_path = \"MICCAI_BraTS_2018_Data_Training.zip\"  # Replace with your dataset path\n","zfile = zipfile.ZipFile(dataset_path)\n","zfile.extractall()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"35DzVL3NFUeu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5vSCj82WFU2u","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"hFVH-pAgFVG7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"09ARH9U4D2Wy"},"source":["## Imports and helper functions"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4uAsBShgB3v_","outputId":"f1b7fe72-b0d7-4f63-ba15-44c16775e725","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import SimpleITK as sitk  # For loading the dataset\n","import numpy as np  # For data manipulation\n","from model import build_model  # For creating the model\n","import glob  # For populating the list of files\n","from scipy.ndimage import zoom  # For resizing\n","import re  # For parsing the filenames (to know their modality)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nNNtloQ9B3sA","colab":{}},"source":["def read_img(img_path):\n","    \"\"\"\n","    Reads a .nii.gz image and returns as a numpy array.\n","    \"\"\"\n","    return sitk.GetArrayFromImage(sitk.ReadImage(img_path))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fEIRGXpdfssT","colab":{}},"source":["def resize(img, shape, mode='constant', orig_shape=(155, 240, 240)):\n","    \"\"\"\n","    Wrapper for scipy.ndimage.zoom suited for MRI images.\n","    \"\"\"\n","    assert len(shape) == 3, \"Can not have more than 3 dimensions\"\n","    factors = (\n","        shape[0]/orig_shape[0],\n","        shape[1]/orig_shape[1], \n","        shape[2]/orig_shape[2]\n","    )\n","    \n","    # Resize to the given shape\n","    return zoom(img, factors, mode=mode)\n","\n","\n","def preprocess(img, out_shape=None):\n","    \"\"\"\n","    Preprocess the image.\n","    Just an example, you can add more preprocessing steps if you wish to.\n","    \"\"\"\n","    if out_shape is not None:\n","        img = resize(img, out_shape, mode='constant')\n","    \n","    # Normalize the image\n","    mean = img.mean()\n","    std = img.std()\n","    return (img - mean) / std\n","\n","\n","def preprocess_label(img, out_shape=None, mode='nearest'):\n","    \"\"\"\n","    Separates out the 3 labels from the segmentation provided, namely:\n","    GD-enhancing tumor (ET — label 4), the peritumoral edema (ED — label 2))\n","    and the necrotic and non-enhancing tumor core (NCR/NET — label 1)\n","    \"\"\"\n","    print(img.shape)\n","    print(np.unique(img))\n","    ncr = img == 1  # Necrotic and Non-Enhancing Tumor (NCR/NET)\n","    ed = img == 2  # Peritumoral Edema (ED)\n","    et = img == 4  # GD-enhancing Tumor (ET)\n","    \n","    if out_shape is not None:\n","        ncr = resize(ncr, out_shape, mode=mode)\n","        ed = resize(ed, out_shape, mode=mode)\n","        et = resize(et, out_shape, mode=mode)\n","\n","    return np.array([ncr, ed, et], dtype=np.uint8)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"piRsc9rYYRzl"},"source":["## Loading Data\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7adjWzdCuECK","colab":{}},"source":["# Get a list of files for all modalities individually\n","t1 = glob.glob('*GG/*/*t1.nii.gz')\n","t2 = glob.glob('*GG/*/*t2.nii.gz')\n","flair = glob.glob('*GG/*/*flair.nii.gz')\n","t1ce = glob.glob('*GG/*/*t1ce.nii.gz')\n","seg = glob.glob('*GG/*/*seg.nii.gz')  # Ground Truth"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"x_WhT40dzlHO"},"source":["Parse all the filenames and create a dictionary for each patient with structure:\n","\n","{<br />\n","    &nbsp;&nbsp;&nbsp;&nbsp;'t1': _<path to t1 MRI file&gt;_,<br />\n","    &nbsp;&nbsp;&nbsp;&nbsp;'t2': _<path to t2 MRI&gt;_,<br />\n","    &nbsp;&nbsp;&nbsp;&nbsp;'flair': _<path to FLAIR MRI file&gt;_,<br />\n","    &nbsp;&nbsp;&nbsp;&nbsp;'t1ce': _<path to t1ce MRI file&gt;_,<br />\n","    &nbsp;&nbsp;&nbsp;&nbsp;'seg': _<path to Ground Truth file&gt;_,<br />\n","}<br />"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4dssK9Nmwojp","colab":{}},"source":["pat = re.compile('.*_(\\w*)\\.nii\\.gz')\n","\n","data_paths = [{\n","    pat.findall(item)[0]:item\n","    for item in items\n","}\n","for items in list(zip(t1, t2, t1ce, flair, seg))]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ba8WFd9GEuXI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wHqqrbnH0n2E"},"source":["## Load the data in a Numpy array\n","Creating an empty Numpy array beforehand and then filling up the data helps you gauge beforehand if the data fits in your memory.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tBgbr-R-7Zat"},"source":["_Loading only the first 4 images here, to save time._"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nvbbddauz8ij","colab":{}},"source":["input_shape = (4, 80, 96, 64)\n","# input_shape = (4, 155, 240, 240)\n","output_channels = 3\n","data = np.empty((len(data_paths[:4]),) + input_shape, dtype=np.float32)\n","labels = np.empty((len(data_paths[:4]), output_channels) + input_shape[1:], dtype=np.uint8)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"G4u_yrMf30k4","colab":{}},"source":["import math\n","\n","# Parameters for the progress bar\n","total = len(data_paths[:4])\n","step = 25 / total\n","\n","for i, imgs in enumerate(data_paths[:4]):\n","    try:\n","        data[i] = np.array([preprocess(read_img(imgs[m]), input_shape[1:]) for m in ['t1', 't2', 't1ce', 'flair']], dtype=np.float32)\n","        labels[i] = preprocess_label(read_img(imgs['seg']), input_shape[1:])[None, ...]\n","        \n","        # Print the progress bar\n","        print('\\r' + 'Progress: ' + \"[%s %s]\"%('=' * int((i+1) * step), ' ' * (24 - int((i+1) * step))) + \"(%s percentage)\"%(math.ceil((i+1) * 100 / (total))),\n","            end='')\n","    except Exception as e:\n","        print('Something went wrong with %s, skipping...\\n Exception:\\n%s'%(imgs[\"t1\"], str(e)))\n","        continue"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"veunXZA7EuXd","colab_type":"code","colab":{}},"source":["labels.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ojc7YgwC305t"},"source":["## Model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uR_SDmgn4Lrd"},"source":["build the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vI53S6yZJN2V","colab":{}},"source":["model = build_model(input_shape=input_shape, output_channels=3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s1u107O74NOP"},"source":["Train the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"b0y7NfU4JBgi","colab":{}},"source":["# model.fit(data, labels, batch_size=1, epochs=5)\n","# preds = model.predict(np.array([data[0]]))\n","import matplotlib.pyplot as plt\n","pred = preds[0]\n","print(pred.shape)\n","print(pred[0,:,:,:].shape)\n","# pred[:, 50,:,:] #50th slice \n","# img = pred.sum(axis=0)\n","# img = img.sum(axis=0)\n","# img = (img>1).astype(np.uint8)\n","# print(img.shape)\n","# print(np.unique(img))\n","# print(img.sum())\n","# plt.imshow(pred[:, 50,:,:], cmap='Greys_r')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VlrqEF7m7ist"},"source":["That's it!"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZKerVpLQTRck"},"source":["## Closing Regards"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bkSxJbRx4SsO"},"source":["If you are resizing the segmentation mask, the resized segmentation mask retains the overall shape, but loses a lot of pixels and becomes somewhat 'grainy'. See the illustration below."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eAau6Z7hznFC"},"source":["1. Original segmentation mask:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CXRW65_hkL9n","colab":{}},"source":["import matplotlib.pyplot as plt\n","img = read_img(seg[0])\n","print(img.shape)\n","img = img.sum(axis=0)\n","img = (img>1).astype(np.uint8)\n","print(img.shape)\n","print(np.unique(img))\n","print(img.sum())\n","plt.imshow(img, cmap='Greys_r')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WCObpjA3EuX_","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","slice_no = 60\n","s = read_img('/home/suraj/Documents/smita/MICCAI_BraTS_2018_Data_Training/LGG/Brats18_TCIA12_101_1/Brats18_TCIA12_101_1_flair.nii.gz')\n","print(s.shape)\n","# s = s.sum(axis=0)\n","# plt.imshow(s[slice_no,:,:], cmap='gray')\n","fig = plt.figure() # make figure\n","\n","# make axesimage object\n","# the vmin and vmax here are very important to get the color map correct\n","im = plt.imshow(s[0,:,:], cmap='gray')\n","\n","# function to update figure\n","def updatefig(j):\n","    # set the data in the axesimage object\n","    print(j)\n","    im.set_array(s[j])\n","    # return the artists set\n","    return [im]\n","# kick off the animation\n","ani = animation.FuncAnimation(fig, updatefig, frames=range(1,155), \n","                              interval=50, blit=True)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7vYIvVRaEuYD","colab_type":"code","colab":{}},"source":["s = read_img('/home/suraj/Documents/smita/MICCAI_BraTS_2018_Data_Training/LGG/Brats18_TCIA12_101_1/Brats18_TCIA12_101_1_t1.nii.gz')\n","print(s.shape)\n","# s = s.sum(axis=0)\n","plt.imshow(s[slice_no], cmap='gray')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Q30U6fREuYM","colab_type":"code","colab":{}},"source":["s = read_img('/home/suraj/Documents/smita/MICCAI_BraTS_2018_Data_Training/LGG/Brats18_TCIA12_101_1/Brats18_TCIA12_101_1_t2.nii.gz')\n","print(s.shape)\n","# s = s.sum(axis=0)\n","plt.imshow(s[slice_no], cmap='gray')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kHQf6FNqEuYQ","colab_type":"code","colab":{}},"source":["s = read_img('/home/suraj/Documents/smita/MICCAI_BraTS_2018_Data_Training/LGG/Brats18_TCIA12_101_1/Brats18_TCIA12_101_1_t1ce.nii.gz')\n","print(s.shape)\n","# s = s.sum(axis=0)\n","plt.imshow(s[slice_no], cmap='gray')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QXgcqjZEuYV","colab_type":"code","colab":{}},"source":["s = read_img('/home/suraj/Documents/smita/MICCAI_BraTS_2018_Data_Training/LGG/Brats18_TCIA12_101_1/Brats18_TCIA12_101_1_seg.nii.gz')\n","print(s.shape)\n","# s = s.sum(axis=0)\n","# s = (s>1).astype(np.uint8)\n","plt.imshow(s[90], cmap='Greys_r')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"soDTIJFn5CE9"},"source":["After resizing to (80, 96, 64)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-kmgpXjsAMzj","colab":{}},"source":["img = preprocess_label(read_img(seg[0]), out_shape=(80, 96, 64), mode='nearest')\n","print(img.shape)\n","img = preprocess_label(read_img(seg[0]), out_shape=(155, 240, 240), mode='nearest')\n","img = img.sum(axis=0)\n","img = img.sum(axis=0)\n","img = (img>1).astype(np.uint8)\n","print(img.shape)\n","print(np.unique(img))\n","print(img.sum())\n","plt.imshow(img, cmap='Greys_r')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FiGSgdqA5LGd"},"source":["One can clearly notice that there are now a lot of black pixels in the region where there should have been only white pixels. This can potentially hurt our model. So, it is best to not resize the image too much. But, due to computational constraints and the model requirements, it is unavoidable. \n","\n","However, given below are a few things one could try to reduce the downsampling noise as much as possible."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"01FtsxQi6dts","colab":{}},"source":["import cv2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vbflvVYqz2yu"},"source":["- Original Image > preprocess_label > Morphological Closing"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vFn6tiXPsUdl","colab":{}},"source":["kernel = np.ones((3, 3))\n","img_closed = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel, iterations=3)\n","print(np.unique(img_closed))\n","print(img_closed.sum())\n","plt.imshow(img_closed, cmap='Greys_r')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"17CKK5zQz7hs"},"source":["- Original Image > preprocess_label > Morphological Dilation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bxhKa3kKtCLZ","colab":{}},"source":["kernel = np.ones((3, 3))\n","img_dilated = cv2.dilate(img, kernel, iterations=1)\n","print(np.unique(img_dilated))\n","print(img_dilated.sum())\n","plt.imshow(img_dilated, cmap='Greys_r')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gG3vfTyS0BVe"},"source":["You could try these things to get even better results."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CX6Cj1VO6vrZ"},"source":["## Feedback"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZPt_15Ps6vO1"},"source":["If you have any feedback, queries, bug reports to send, please feel free to [raise an issue](https://github.com/IAmSuyogJadhav/3d-mri-brain-tumor-segmentation-using-autoencoder-regularization/issues/new) on github. It would be really helpful!"]}]}